<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RnG</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2>RnG: A Unified Transformer for Complete <br>3D Modeling from Partial Observations</h2>
          <h4 style="color:#5a6268;">CVPR 26</h4>
          <hr>
          <!-- <h6>
            <a href="http://npu-cvr.cn/" target="_blank">Jiahui Ren</a><sup>⭐️</sup>,
            <a href="https://scholar.google.com/citations?user=OC5oCTgAAAAJ" target="_blank">Mochu Xiang</a><sup>⭐️</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Jiajun Zhu</a>,
            <a href="https://scholar.google.com/citations?user=fddAbqsAAAAJ" target="_blank">Yuchao Dai</a><sup>✉️</sup>,
          </h6>
          <p> School of Electronics and Information, Northwestern Polytechnical University and
            <br>Shaanxi Key Laboratory of Information Acquisition and Processing, Xi'an, Shaanxi, China
            <br>
            <br>
            <a style="color:#9b9fa2">
              <sup>⭐️</sup> denotes equal contribution &nbsp;&nbsp;
              <sup>✉️</sup> Corresponding author &nbsp; <a style="font-family: 'Courier New', monospace; color: #9abad3;"> daiyuchao[at]nwpu.edu.cn</a>
            </a>
          </p> -->


          <div class="row justify-content-center">
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" role="button"
                  target="_blank">
                  <i class="fa fa-file"></i> Paper coming soon</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" role="button" target="_blank">
                  <i class="fa fa-github-alt"></i> Code coming soon</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" role="button" target="_blank">
                  &#129303; Model weights & Data coming soon</a> </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- abstract -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Abstract</h3>
        <hr style="margin-top:0px">
        <p class="text-left">
          Human perceive the 3D world through 2D observations from limited viewpoints. 
          While recent feed-forward generalizable 3D reconstruction models excel at recovering 3D structures from sparse images, 
          their representations are often confined to observed regions, leaving unseen geometry un-modeled. 
          This raises a key, fundamental challenge: Can we infer a complete 3D structure from partial 2D observations? 
          We present RnG (Reconstruction and Generation), a novel feed-forward Transformer that unifies these two tasks by predicting an implicit, complete 3D representation. 
          At the core of RnG, we propose a reconstruction-guided causal attention mechanism that separates reconstruction and generation at the attention level, and treats the KV-cache as an implicit 3D representation. 
          Then, arbitrary poses can efficiently query this cache to render high-fidelity, novel-view RGBD outputs. 
          As a result, RnG not only accurately reconstructs visible geometry but also generates plausible, coherent unseen geometry and appearance. 
          Our method achieves state-of-the-art performance in both generalizable 3D reconstruction and novel view generation, while operating efficiently enough for real-time interactive applications.
        </p>
      </div>
    </div>
  </div>
</section>
<br>


<!-- showcase -->
<!-- <section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>PanoSplatt3R Architecture</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="./imgs/PanoSplatt3R_architecture.png" alt="Architecture">
        <p><strong>Overview of our proposed PanoSplatt3R.</strong> 
          Given two panorama images as input and without knowing their relative pose, 
          our model reconstructs the entire 3D scene using Gaussian Splats. 
          The estimated geometry can provide photorealistic renderings at novel view points. 
          We removed two walls from the final reconstruction to better display the room.</p>
      </div>
    </div>
  </div>
</section>
<br> -->


<!-- comparison -->
<!-- <section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Comparison with state-of-the-art methods</h3>
        <hr style="margin-top:0px">
        <p><strong>Quantitative comparisons on wide-baseline panorama reconstructions.</strong>
          We compare our model with methods that require accurate known pose, 
          including those with cube map inputs and panorama inputs. 
          Our model, even in the absence of camera pose, 
          presents the best performance in both novel view synthesis quality and depth estimation accuracy.</p>
        <img class="img-fluid" width="100%" src="./imgs/comparison.png" alt="Performance">
      </div>
    </div>
  </div>
</section>
<br> -->

<!-- visual comparison video -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Visual Comparison</h3>
        <hr style="margin-top:0px">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe width="950" height="534" src="https://youtube.com/embed/l1X6U8xiMyE" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<br>

<!-- citing -->
<!-- <div class="container">
  <div class="row ">
    <div class="col-12">
      <h3>Citation</h3>
      <hr style="margin-top:0px">
      <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{PanoSplatt3R,
  title={PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction},
  author={Ren, Jiahui and Xiang, Mochu and Zhu, Jiajun and Dai, Yuchao},
  booktitle={ICCV},
  year={2025}
}
</code></pre>
      <hr>
    </div>
  </div>
</div> -->

<footer class="text-center" style="margin-bottom:10px">
  <br>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
</footer>

</body>

</html>