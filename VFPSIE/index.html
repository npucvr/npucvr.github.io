<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Video Frame Prediction from a Single Image and Events</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2>Video Frame Prediction from a Single Image and Events</h2>
          <h4 style="color:#5a6268;">AAAI Conference on Artificial Intelligence,2024</h4>
          <hr>
          <h6>
            <a href="http://npu-cvr.cn/" target="_blank">Juanjuan Zhu</a><sup>1*</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Zhexiong Wan</a><sup>1*</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Yuchao Dai</a><sup>#</sup>,
          </h6>
          <p><sup>1</sup>Northwestern Polytechnical University <br/>
            <sup>*</sup> denotes equal contribution <br/>
            <sup>#</sup> corresponding author <br/> 
            juanjuanzhu2022@mail.nwpu.edu.cn, wanzhexiong@mail.nwpu.edu.cn, daiyuchao@nwpu.edu.cn
          </p>


          <div class="row justify-content-center">
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="Zhu_Video_Frame_Prediction_From_A_Single_Image_And_Events.pdf" role="button"
                  target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/Gwynplainyg/VFPSIE" role="button" target="_blank">
                  <i class="fa fa-github-alt"></i> Code</a> </p>
            </div>
            <!-- <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://xxx" role="button">
                  <i class="fa fa-file"></i> Supplementary </a> </p>
            </div> -->
            <!-- <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button">
                  <i class="fa fa-database"></i> Data</a> </p>
            </div> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- abstract -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Abstract</h3>
        <hr style="margin-top:0px">
        <p class="text-left"> Recently, the task of Video Frame Prediction (VFP), which predicts future video frames from previous ones through extrapolation, has made remarkable progress. 
          However, the performance of existing VFP methods is still far from satisfactory due to the fixed framerate video used: 1) they have difficulties in handling <b>complex dynamic scenes</b>; 2) they cannot predict future frames with <b>flexible prediction time intervals</b>. 
          The event cameras can record the intensity changes asynchronously with a very high temporal resolution, which provides rich dynamic information about the observed scenes. 
          In this paper, we propose to predict video frames from <b>a single image and the following events</b>, which can not only handle <b>complex dynamic scenes</b> but also predict future frames with <b>flexible prediction time intervals</b>.
          First, we introduce a symmetrical cross-modal attention augmentation module to enhance the complementary information between images and events. 
          Second, we propose to jointly achieve optical flow estimation and frame generation by combining the motion information of events and the semantic information of the image, then inpainting the holes produced by forward warping to obtain an ideal prediction frame.
          Based on these, we propose a lightweight pyramidal coarse-to-fine model that can predict a 720P frame within 25 ms.
          Extensive experiments show that our proposed model significantly outperforms the state-of-the-art frame-based and event-based VFP methods and has the fastest runtime.  </p>
      </div>
    </div>
  </div>
</section>
<br>

<!-- overview video -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Overview Video</h3>
        <hr style="margin-top:0px">
        <div class="embed-responsive embed-responsive-16by9">
          <iframe width="950" height="534" src="https://www.youtube.com/embed/b7HXFB6WhGk?si=_i8sSyPiPBDsRc3j" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<br>

<!-- showcase -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Model Architecture</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="images/total_model.png" alt="Architecture">
        <p class="text-left"> In our framework, we first use two encoders to extract pyramid features for the image and events.
          Then we apply a coarse-to-fine joint decoder to get the synthesized feature and optical flow at each pyramid layer. In the decoder, we utilize <b>Symmetrical Cross-modal Attention(SCA)</b> to augment both image and event features.
          We also introduce <b>Warping and Inpainting Module(WIM)</b> to repair the holes caused by forward warping and get spatially-aligned image features. 
          Finally, we adopt <b>Weighted Fusion(WF)</b> to output the final frame prediction from the synthesized and warped frames. </p>
      </div>
    </div>
  </div>
</section>
<br>

<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Performance Comparison</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="95%" src="images/Table.png" alt="Performance comparison">
        <p class="text-left">  </p>
      </div>
    </div>
  </div>
</section>
<br>

<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Visual Comparison</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="95%" src="images/comparison.png" alt="Visual comparison">
      </div>
    </div>
  </div>
</section>
<br>


<!-- comparison -->
<!-- <section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Comparison with state-of-the-art methods</h3>
        <hr style="margin-top:0px">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source src="./videos/xxx.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>
<br> -->

<!-- citing -->
<div class="container">
  <div class="row ">
    <div class="col-12">
      <h3>Citation</h3>
      <hr style="margin-top:0px">
      <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{Zhu_VFPSIE_AAAI_2024,
  title={Video Frame Prediction from a Single Image and Events},
  author={Zhu, Juanjuan and Wan, Zhexiong and Dai, Yuchao},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2024},
} </code></pre>
      <hr>
    </div>
  </div>
</div>

<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Acknowledgments</h3>
        <hr style="margin-top:0px">

        <p class="text-left"> This research was supported in part by the National Natural Science Foundation of China (62271410, 62001394), Zhejiang Lab (NO.2021MC0AB05), the Fundamental Research Funds for the Central Universities, and the Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University (CX2023013).</p>
        <p class="text-left"> Thanks the ACs and the reviewers for their comments, which is very helpful to improve our paper. </p>

        <p class="text-left"> Thanks for the following helpful open source projects: 
          <a href="https://github.com/uzh-rpg/rpg_timelens" target="_blank">Time Lens</a>, 
          <a href="https://github.com/ltkong218/IFRNet" target="_blank">IFRNet</a>, 
          <a href="https://github.com/princeton-vl/RAFT" target="_blank">RAFT</a>, 
          <a href="https://github.com/uzh-rpg/rpg_vid2e" target="_blank">esim_py</a>, 
          <a href="https://github.com/uzh-rpg/DSEC" target="_blank">DSEC</a>.
      </div>
    </div>
  </div>
</section>
<br>

<footer class="text-center" style="margin-bottom:10px">
  <br>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
</footer>

</body>

</html>
